{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmlOz93X9k0b63evn3f/5f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YCCS-Summer-2023-DDNMA/project/blob/83-implementing-first-nn-as-practice-JC/Joseph_Couzens/notebooks/PosNegSentimentNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build a sentiment analysis neural network for IMDb movie reviews, you can follow these general steps:\n",
        "\n",
        "Dataset Preparation:\n",
        "Download the IMDb movie reviews dataset from Kaggle or any other reliable source.\n",
        "Preprocess the dataset by cleaning and transforming the text data. This may involve removing special characters, lowercasing the text, and tokenizing the reviews into individual words or subwords.\n",
        "Split the dataset into training and testing sets. Typically, you reserve a portion of the dataset (e.g., 80%) for training the model and the remaining portion for evaluating its performance.\n",
        "\n",
        "Text Representation:\n",
        "Convert the processed text into numerical representations that can be fed into the neural network.\n",
        "One common approach is to use word embeddings such as Word2Vec or GloVe, which map words to dense vectors. Alternatively, you can utilize techniques like TF-IDF or bag-of-words representation.\n",
        "Ensure that all the input data has the same shape and length by padding or truncating the sequences as needed.\n",
        "\n",
        "Model Architecture:\n",
        "Choose the type of neural network architecture suitable for sentiment analysis, such as a recurrent neural network (RNN), long short-term memory (LSTM), or a 1D convolutional neural network (CNN).\n",
        "Stack layers and define the structure of your neural network. This can include embedding layers (if not using pre-trained word embeddings), recurrent or convolutional layers, and dense layers.\n",
        "Experiment with the number of layers, hidden units, and activation functions to find the right balance between model complexity and performance.\n",
        "\n",
        "Training and Evaluation:\n",
        "Compile the model by specifying the loss function (e.g., binary cross-entropy) and the optimizer (e.g., Adam or SGD) to use during training.\n",
        "Train the model on the training dataset using the fit() function or similar methods. Monitor the training process by observing metrics like accuracy and loss.\n",
        "Validate the model on the testing dataset to evaluate its performance and generalization.\n",
        "Fine-tune hyperparameters and model architecture as necessary based on the evaluation results.\n",
        "\n",
        "Prediction:\n",
        "Use the trained model to predict sentiment on new, unseen movie reviews or user-provided text.\n",
        "Process the input text in the same way as during the training phase (cleaning, tokenizing, padding, etc.).\n",
        "Pass the preprocessed text through the trained model and obtain the predicted sentiment."
      ],
      "metadata": {
        "id": "Vaw9OniSEV5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ml-collections & latest Flax version from Github.\n",
        "!pip install -q ml-collections git+https://github.com/google/flax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r5_YDhIvxWV",
        "outputId": "622763db-2a17-44ac-c5fa-98d58194ac23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "FihEtkb4rlMf"
      },
      "outputs": [],
      "source": [
        "from absl import logging\n",
        "from flax import linen as nn\n",
        "from flax.metrics import tensorboard\n",
        "from flax.training import train_state\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import ml_collections\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function just used for splitting the dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import nltk #to get stop words to avoid for selecting vocab and sentiment words\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "def create_vocabulary(text_data):\n",
        "  # Convert the NumPy array to a Pandas Series so it can be split like str\n",
        "  text_series = pd.Series(text_data)\n",
        "  # Tokenize the text data\n",
        "  tokenized_text = text_series.str.split()\n",
        "\n",
        "  # Remove stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokenized_text = tokenized_text.apply(lambda x: [word for word in x if word not in stop_words])\n",
        "\n",
        "  # Perform frequency analysis\n",
        "  word_frequencies = tokenized_text.explode().value_counts()\n",
        "\n",
        "  # Manually select sentiment-related words\n",
        "  sentiment_related_words = ['good', 'bad', 'excellent', 'horrible', 'amazing', 'terrible', 'lacking',\n",
        "                             'excellent', 'impressive', 'poor', 'interesting', 'well',\n",
        "                             'great', 'fun', 'worst', 'best', 'lacking', 'funny', 'boring', 'uninteresting', 'not',\n",
        "                             'amazing','awesome', 'brilliant', 'captivating', 'charming', 'delightful','engaging',\n",
        "                             'enjoyable', 'entertaining', 'excellent', 'exceptional', 'exciting', 'fantastic', 'fascinating',\n",
        "                             'heartwarming', 'impressive', 'incredible', 'inspiring',\n",
        "                             'joyful', 'lovely', 'marvelous', 'masterful', 'outstanding', 'phenomenal', 'pleasing', 'remarkable',\n",
        "                             'riveting', 'spectacular', 'splendid', 'stunning', 'superb', 'terrific', 'thrilling', 'wonderful',\n",
        "                             'awful', 'boring', 'disappointing', 'dreadful','dull', 'fails', 'forgettable', 'frustrating', 'lackluster', 'mediocre', 'messy', 'painful', 'poor',\n",
        "                             'predictable', 'ridiculous', 'slow', 'stupid', 'terrible', 'trite', 'underwhelming', 'uninspiring', 'uninteresting', 'unoriginal', 'weak',]  # Add more as needed\n",
        "\n",
        "  # Create the vocabulary\n",
        "  vocabulary = set(sentiment_related_words)\n",
        "\n",
        "  return vocabulary\n",
        "\n",
        "def create_bow_vectors(text_data, vocabulary):\n",
        "  bow_vectors = []\n",
        "  for document in text_data:\n",
        "      word_frequency = {word: 0 for word in vocabulary}\n",
        "      for word in document:\n",
        "          if word in vocabulary:\n",
        "              word_frequency[word] += 1\n",
        "      bow_vectors.append(list(word_frequency.values()))\n",
        "  return bow_vectors\n",
        "\n",
        "mx_seq_len = 0 #for later reference in create train state\n",
        "def get_datasets():\n",
        "  global mx_seq_len\n",
        "\n",
        "  file_path = 'IMDB_Dataset.csv'  # Relative file path\n",
        "  data = pd.read_csv(file_path, error_bad_lines=False)\n",
        "  train_data, test_data = train_test_split(data, test_size=0.5, random_state=42)\n",
        "\n",
        "  vocabulary = create_vocabulary(train_data['review'].values)\n",
        "\n",
        "  train_bow_vectors = create_bow_vectors(train_data['review'].values, vocabulary)\n",
        "  test_bow_vectors = create_bow_vectors(test_data['review'].values, vocabulary)\n",
        "\n",
        "  # Determine the maximum sequence length\n",
        "  max_sequence_length = max(len(vector) for vector in train_bow_vectors + test_bow_vectors)\n",
        "  mx_seq_len = max_sequence_length\n",
        "\n",
        "  # Pad the sequences\n",
        "  train_bow_vectors = pad_sequences(train_bow_vectors, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "  test_bow_vectors = pad_sequences(test_bow_vectors, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "  train_labels = train_data['sentiment'].values\n",
        "  test_labels = test_data['sentiment'].values\n",
        "\n",
        "  #now pass these vectors into the network\n",
        "  return train_bow_vectors, train_labels, test_bow_vectors, test_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZvIByFDr_ZF",
        "outputId": "e919376a-5ce8-464e-8fd7-d46407b3b626"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now attempting to apply the CNN from train.py\n",
        "class sentimentCNN(nn.Module):\n",
        "  \"\"\"A BoW sentiment CNN model.\"\"\"\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "    x = x.reshape((x.shape[0], -1))  # flatten\n",
        "    x = nn.Dense(features=256)(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.Dense(features=10)(x)\n",
        "    return x\n",
        "\n",
        "@jax.jit\n",
        "def apply_model(state, input, labels):\n",
        "  \"\"\"Computes gradients, loss and accuracy for a single batch.\"\"\"\n",
        "  def loss_fn(params):\n",
        "    logits = state.apply_fn({'params': params}, input)\n",
        "    one_hot = jax.nn.one_hot(labels, 10)\n",
        "    loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
        "    return loss, logits\n",
        "\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, logits), grads = grad_fn(state.params)\n",
        "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
        "  return grads, loss, accuracy\n",
        "\n",
        "@jax.jit\n",
        "def update_model(state, grads):\n",
        "  return state.apply_gradients(grads=grads)\n",
        "\n",
        "def train_epoch(state, train_ds, train_labels, batch_size, rng):\n",
        "  \"\"\"Train for a single epoch.\"\"\"\n",
        "  train_ds_size = len(train_ds)\n",
        "  steps_per_epoch = train_ds_size // batch_size\n",
        "\n",
        "  perms = jax.random.permutation(rng, len(train_ds))\n",
        "  perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch\n",
        "  perms = perms.reshape((steps_per_epoch, batch_size))\n",
        "\n",
        "  epoch_loss = []\n",
        "  epoch_accuracy = []\n",
        "\n",
        "  for perm in perms:\n",
        "    batch_vectors = train_ds[perm, ...]\n",
        "    batch_labels = train_labels[perm, ...]\n",
        "    grads, loss, accuracy = apply_model(state, batch_vectors, batch_labels)\n",
        "    state = update_model(state, grads)\n",
        "    epoch_loss.append(loss)\n",
        "    epoch_accuracy.append(accuracy)\n",
        "  train_loss = np.mean(epoch_loss)\n",
        "  train_accuracy = np.mean(epoch_accuracy)\n",
        "  return state, train_loss, train_accuracy\n",
        "\n",
        "\n",
        "def create_train_state(rng, config):\n",
        "    \"\"\"Creates initial `TrainState`.\"\"\"\n",
        "    cnn = sentimentCNN()\n",
        "    params = cnn.init(rng, jnp.ones([1, mx_seq_len]))['params']  # Adjust the input shape based on your BoW vector shape\n",
        "    tx = optax.sgd(config.learning_rate, config.momentum)\n",
        "    return train_state.TrainState.create(\n",
        "        apply_fn=cnn.apply, params=params, tx=tx)\n",
        "\n",
        "\n",
        "\n",
        "def train_and_evaluate(config: ml_collections.ConfigDict, workdir: str) -> train_state.TrainState:\n",
        "    \"\"\"Execute model training and evaluation loop.\n",
        "\n",
        "    Args:\n",
        "        config: Hyperparameter configuration for training and evaluation.\n",
        "        workdir: Directory where the tensorboard summaries are written to.\n",
        "\n",
        "    Returns:\n",
        "        The train state (which includes the `.params`).\n",
        "    \"\"\"\n",
        "    train_ds, train_labels, test_ds, test_labels = get_datasets()\n",
        "    rng = jax.random.PRNGKey(0)\n",
        "\n",
        "    summary_writer = tensorboard.SummaryWriter(workdir)\n",
        "    summary_writer.hparams(dict(config))\n",
        "\n",
        "    rng, init_rng = jax.random.split(rng)\n",
        "    state = create_train_state(init_rng, config)\n",
        "\n",
        "    for epoch in range(1, config.num_epochs + 1):\n",
        "        rng, input_rng = jax.random.split(rng)\n",
        "        state, train_loss, train_accuracy = train_epoch(state, train_ds, train_labels, config.batch_size, input_rng)\n",
        "        _, test_loss, test_accuracy = apply_model(state, test_ds, test_labels)\n",
        "\n",
        "        logging.info(\n",
        "            'epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, test_loss: %.4f, test_accuracy: %.2f'\n",
        "            % (epoch, train_loss, train_accuracy * 100, test_loss, test_accuracy * 100))\n",
        "\n",
        "        summary_writer.scalar('train_loss', train_loss, epoch)\n",
        "        summary_writer.scalar('train_accuracy', train_accuracy, epoch)\n",
        "        summary_writer.scalar('test_loss', test_loss, epoch)\n",
        "        summary_writer.scalar('test_accuracy', test_accuracy, epoch)\n",
        "\n",
        "    summary_writer.flush()\n",
        "    return state\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1FqLI5ngOOQv"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now lets attempt to actually use the model\n",
        "\n",
        "config = ml_collections.ConfigDict()\n",
        "config.num_epochs = 5\n",
        "config.batch_size = 32\n",
        "config.learning_rate = 0.001\n",
        "import os\n",
        "workdir = os.getcwd()  # Get the current working directory\n",
        "\n",
        "\n",
        "train_and_evaluate(config, workdir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "-jCJVPQfab54",
        "outputId": "65b281b9-0727-48df-86a7-7b2745aa2496"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-a429efa7424b>:54: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  data = pd.read_csv(file_path, error_bad_lines=False)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-554671590be9>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-ebb13ee7bf39>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(config, workdir)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_rng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_train_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-ebb13ee7bf39>\u001b[0m in \u001b[0;36mcreate_train_state\u001b[0;34m(rng, config)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;34m\"\"\"Creates initial `TrainState`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentimentCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmx_seq_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Adjust the input shape based on your BoW vector shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     return train_state.TrainState.create(\n",
            "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-ebb13ee7bf39>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flax/linen/linear.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpromote_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m       y = self.conv_general_dilated(\n\u001b[0m\u001b[1;32m    450\u001b[0m           \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m           \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_ceil_divide\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m   4472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4473\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_ceil_divide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4474\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor_divide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (0,) (2,) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now lets attempt to actually use the model\n",
        "example_directory = 'examples/mnist'\n",
        "editor_relpaths = ('configs/default.py')\n",
        "\n",
        "repo, branch = 'https://github.com/google/flax', 'main'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from configs import default as config_lib\n",
        "config = config_lib.get_config()\n",
        "\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir=.\n",
        "\n",
        "\n",
        "config.num_epochs = 3\n",
        "models = {}\n",
        "for momentum in (0.8, 0.9, 0.95):\n",
        "  name = f'momentum={momentum}'\n",
        "  config.momentum = momentum\n",
        "  state = train_and_evaluate(config, workdir=f'./models/{name}')\n",
        "  models[name] = state.params\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "9pr6yG_dWfBZ",
        "outputId": "7a52f29e-b780-4c1b-84e6-9c5e2244ed9b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-cc0e22b0e88d>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconfigs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconfig_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'configs'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}